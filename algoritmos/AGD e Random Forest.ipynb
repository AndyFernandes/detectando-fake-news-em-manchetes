{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log, pi, sqrt, exp\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "#import pydotplus -> baixar esse cara\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x, y):\n",
    "    classes, ocorrencs = np.unique(y, return_counts=True) # pegando as classes e ocorrencias\n",
    "    numClasses = len(classes) # numero de classes\n",
    "    n = len(y) # numero de linhas do dataset\n",
    "    numFeatures = x.shape[1] # numero de colunas do dataset\n",
    "    \n",
    "    # Probabilidade das classes será proporcional a frequencia dessa classe no dataset\n",
    "    probabilidadeClasses = dict(zip(uniques, countOcorrenc))\n",
    "    for key in probabilidadeClasses:\n",
    "        probabilidadeClasses[key] = probabilidadeClasses[key] / n\n",
    "    \n",
    "    media = np.zeros((numFeatures, numClasses)) # criando a lista da media das features por classe\n",
    "    covar = np.zeros((numFeatures, numFeatures, numClasses)) # criando a lista de matrizes de correlação das features para cada classe\n",
    "    for classe in classes:\n",
    "        xk = x[y == classe] # pega as linhas em que classe é igual ao y -> que aí eu acesso essas linhas do x\n",
    "        media[:, classe] = np.mean(xk, axis=0)\n",
    "        xi_mean = xk - mean\n",
    "        covar[:, :, classe] = (xi_mean @ np.transpose(xi_mean))/len(xk)\n",
    "        covar[:, :, classe] = np.mean(np.diag(covar[:,:,classe])) # ver se assim ta certo, pq o do cesar ele botou uns numeros que eu nao entendi\n",
    "    return {'media': media, 'covar': covar, 'classes': classes, 'numRows': n, 'numClasses': numClasses, 'numFeatures': numFeatures, 'probabilidadeClasses': probabilidadeClasses }\n",
    "        \n",
    "def predict(model, row):\n",
    "    probabilits = np.zeros(model['numClasses'])\n",
    "    for classe in model['classes']:\n",
    "        fator1 = 1/(sqrt(np.linalg.det(model['covar'][:, :, classe])) * ((2*pi)**(model['numRows']/2)))\n",
    "        \n",
    "        inversa = np.linalg.inv(model['covar'][:, :, classe])\n",
    "        difXMedia = row - model['media'][:, classe]\n",
    "        z = (-0.5) * (np.transpose(difXMedia) @ inversa @ difXMedia) # valor que fica dentro do exp\n",
    "        probabilits[classe] = fator1 * exp(z)\n",
    "    \n",
    "    \n",
    "# Função para um vetor de testes e então chama o predict\n",
    "def getPredict(model, x_test):\n",
    "    predicted = np.array([predict(model, row) for row in x_test])\n",
    "    return predicted\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = [[1,1,4], [2,2,3]]\n",
    "uniques, countOcorrenc = np.unique(a, return_counts=True)\n",
    "probs = dict(zip(uniques, countOcorrenc))\n",
    "print(probs)\n",
    "\n",
    "n = len(a)\n",
    "for key in probs:\n",
    "    probs[key] = probs[key] / n\n",
    "print(probs)\n",
    "\n",
    "np.mean(a, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# kf = KFold(n_splits = 5, shuffle = True)\n",
    "# ac = [];\n",
    "# x = [1,5,10]\n",
    "# #for i in x:\n",
    "#     # Na linha abaixo, os parâmetros que passei para meu classificador foram, respectivamente:\n",
    "#     # n_estimators (número de árvores), critério para avaliar a qualidade do split, máximo de colunas consideradas\n",
    "#     # em cada split (raiz quadrada do número de colunas, que dá aprox. 7 features), profund. máx das árvores (10)\n",
    "#     clf = RandomForestClassifier(i,\"gini\",7,10)\n",
    "#     for result in kf.split(df):\n",
    "#         train = df.iloc[result[0]]\n",
    "#         test =  df.iloc[result[1]]\n",
    "#         target_train = train[61]\n",
    "#         target_test = test[61]\n",
    "#         train = train.iloc[:, :59]\n",
    "#         test = test.iloc[:, :59]\n",
    "#         clf.fit(train, target_train)\n",
    "#         target_pred = clf.predict(test)\n",
    "#         ac.append(accuracy_score(target_test, target_pred))\n",
    "#     print(\"Árvores: %d\" %i)\n",
    "#     print(\"Scores: \")\n",
    "#     print(ac)\n",
    "#     print(\"Acurácia: %f \" %(mean(ac)))\n",
    "#     print(\"\\n\")\n",
    "#     ac = []\n",
    "\n",
    "\n",
    "\n",
    "num_arvores = 10\n",
    "max_columns = 3\n",
    "max_profundidade = 10\n",
    "clf = RandomForestClassifier(num_arvores,\"gini\",max_columns,max_profundidade)\n",
    "clf.fit(train, target_train)\n",
    "clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
