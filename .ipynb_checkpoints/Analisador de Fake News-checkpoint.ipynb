{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisador de Fake News\n",
    "\n",
    "\n",
    "///////////////// Introdução\n",
    "//////////////// Sumário\n",
    "# OLHAR ESSES LINKS****\n",
    "- seguir esse https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a - ESSE DAQUI TEM O PRECISION/RECALL/ TRATAMENTO DOS DADOS DO INICIO AO FIM OLHAR ELE: https://stackabuse.com/text-classification-with-python-and-scikit-learn/\n",
    "\n",
    "\n",
    "\n",
    "1. Importações\n",
    "2. Leitura dos Dados\n",
    "3. Limpeza e Transformações dos Dados\n",
    "4. Redução de Dimensionalidade dos Dados\n",
    "5. Visualizações dos dados\n",
    "6. Avaliação dos Modelos\n",
    "7. Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from statistics import mean\n",
    "from math import log, pi, sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "# Tratamento do Texto\n",
    "from textblob import TextBlob, Word\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from scipy.linalg import svd \n",
    "from sklearn import linear_model, datasets, svm, metrics\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.neighbors import DistanceMetric, KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Leitura dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URLs</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.bbc.com/news/world-us-canada-414191...</td>\n",
       "      <td>Four ways Bob Corker skewered Donald Trump</td>\n",
       "      <td>Image copyright Getty Images\\r\\nOn Sunday morn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.reuters.com/article/us-filmfestiva...</td>\n",
       "      <td>Linklater's war veteran comedy speaks to moder...</td>\n",
       "      <td>LONDON (Reuters) - “Last Flag Flying”, a comed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.nytimes.com/2017/10/09/us/politics...</td>\n",
       "      <td>Trump’s Fight With Corker Jeopardizes His Legi...</td>\n",
       "      <td>The feud broke into public view last week when...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.reuters.com/article/us-mexico-oil-...</td>\n",
       "      <td>Egypt's Cheiron wins tie-up with Pemex for Mex...</td>\n",
       "      <td>MEXICO CITY (Reuters) - Egypt’s Cheiron Holdin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.cnn.com/videos/cnnmoney/2017/10/08/...</td>\n",
       "      <td>Jason Aldean opens 'SNL' with Vegas tribute</td>\n",
       "      <td>Country singer Jason Aldean, who was performin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                URLs  \\\n",
       "0  http://www.bbc.com/news/world-us-canada-414191...   \n",
       "1  https://www.reuters.com/article/us-filmfestiva...   \n",
       "2  https://www.nytimes.com/2017/10/09/us/politics...   \n",
       "3  https://www.reuters.com/article/us-mexico-oil-...   \n",
       "4  http://www.cnn.com/videos/cnnmoney/2017/10/08/...   \n",
       "\n",
       "                                            Headline  \\\n",
       "0         Four ways Bob Corker skewered Donald Trump   \n",
       "1  Linklater's war veteran comedy speaks to moder...   \n",
       "2  Trump’s Fight With Corker Jeopardizes His Legi...   \n",
       "3  Egypt's Cheiron wins tie-up with Pemex for Mex...   \n",
       "4        Jason Aldean opens 'SNL' with Vegas tribute   \n",
       "\n",
       "                                                Body  Label  \n",
       "0  Image copyright Getty Images\\r\\nOn Sunday morn...      1  \n",
       "1  LONDON (Reuters) - “Last Flag Flying”, a comed...      1  \n",
       "2  The feud broke into public view last week when...      1  \n",
       "3  MEXICO CITY (Reuters) - Egypt’s Cheiron Holdin...      1  \n",
       "4  Country singer Jason Aldean, who was performin...      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"data/data.csv\") # será utilizado para computar coisas nas visualizações\n",
    "dataset['Body'] = dataset['Body'].astype(str) \n",
    "data = dataset # será limpado e utilizado nos modelos\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Limpeza dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URLs</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.bbc.com/news/world-us-canada-414191...</td>\n",
       "      <td>four way bob corker skewered donald</td>\n",
       "      <td>image copyright getty image sunday morning don...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.reuters.com/article/us-filmfestiva...</td>\n",
       "      <td>linklaters war veteran comedy speaks modern am...</td>\n",
       "      <td>london reuters last flag flying comedydrama vi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.nytimes.com/2017/10/09/us/politics...</td>\n",
       "      <td>trump fight corker jeopardizes legislative agenda</td>\n",
       "      <td>feud broke public view last week mr corker mr ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.reuters.com/article/us-mexico-oil-...</td>\n",
       "      <td>egypt cheiron win tieup pemex mexican onshore ...</td>\n",
       "      <td>mexico city reuters egypt cheiron holding limi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.cnn.com/videos/cnnmoney/2017/10/08/...</td>\n",
       "      <td>jason aldean open snl tribute</td>\n",
       "      <td>country singer jason aldean performing la vega...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                URLs  \\\n",
       "0  http://www.bbc.com/news/world-us-canada-414191...   \n",
       "1  https://www.reuters.com/article/us-filmfestiva...   \n",
       "2  https://www.nytimes.com/2017/10/09/us/politics...   \n",
       "3  https://www.reuters.com/article/us-mexico-oil-...   \n",
       "4  http://www.cnn.com/videos/cnnmoney/2017/10/08/...   \n",
       "\n",
       "                                            Headline  \\\n",
       "0                four way bob corker skewered donald   \n",
       "1  linklaters war veteran comedy speaks modern am...   \n",
       "2  trump fight corker jeopardizes legislative agenda   \n",
       "3  egypt cheiron win tieup pemex mexican onshore ...   \n",
       "4                      jason aldean open snl tribute   \n",
       "\n",
       "                                                Body  Label  \n",
       "0  image copyright getty image sunday morning don...      1  \n",
       "1  london reuters last flag flying comedydrama vi...      1  \n",
       "2  feud broke public view last week mr corker mr ...      1  \n",
       "3  mexico city reuters egypt cheiron holding limi...      1  \n",
       "4  country singer jason aldean performing la vega...      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deixar tudo em lower case\n",
    "data['Headline'] = data['Headline'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "data['Body'] = data['Body'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
    "# Removendo pontuação\n",
    "data['Headline'] = data['Headline'].str.replace('[^\\w\\s]','')\n",
    "data['Body'] = data['Body'].str.replace('[^\\w\\s]','')\n",
    "# Removendo stopwords\n",
    "data['Headline'] = data['Headline'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "data['Body'] = data['Body'].apply(lambda x: \" \".join(x for x in str(x).split() if str(x) not in stop))\n",
    "# 10 Palavras mais frequentes e menos frequentes\n",
    "freqHeadline = pd.Series(' '.join(data['Headline']).split()).value_counts()[:10]\n",
    "freqBody = pd.Series(' '.join(data['Body']).split()).value_counts()[:10]\n",
    "rareHead = pd.Series(' '.join(data['Headline']).split()).value_counts()[-10:]\n",
    "rareBody = pd.Series(' '.join(data['Body']).split()).value_counts()[-10:]\n",
    "# Remoção das palavras raras e frequentes\n",
    "freqHeadline = list(freqHeadline.index)\n",
    "freqBody = list(freqBody.index)\n",
    "data['Headline'] = data['Headline'].apply(lambda x: \" \".join(x for x in x.split() if x not in freqHeadline))\n",
    "data['Body'] = data['Body'].apply(lambda x: \" \".join(x for x in x.split() if x not in freqBody))\n",
    "data['Headline'] = data['Headline'].apply(lambda x: \" \".join(x for x in x.split() if x not in rareHead))\n",
    "data['Body'] = data['Body'].apply(lambda x: \" \".join(x for x in x.split() if x not in rareBody))\n",
    "# Lemmatization. Extrai o núcleo/root da palavra\n",
    "data['Headline'] = data['Headline'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "data['Body'] = data['Body'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho:  (4009, 6634)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>four</th>\n",
       "      <th>way</th>\n",
       "      <th>bob</th>\n",
       "      <th>corker</th>\n",
       "      <th>skewered</th>\n",
       "      <th>donald</th>\n",
       "      <th>linklaters</th>\n",
       "      <th>war</th>\n",
       "      <th>veteran</th>\n",
       "      <th>comedy</th>\n",
       "      <th>...</th>\n",
       "      <th>tragic</th>\n",
       "      <th>hervé</th>\n",
       "      <th>leroux</th>\n",
       "      <th>léger</th>\n",
       "      <th>bandage</th>\n",
       "      <th>dress</th>\n",
       "      <th>30minute</th>\n",
       "      <th>100000</th>\n",
       "      <th>antiamerican</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6635 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   four  way  bob  corker  skewered  donald  linklaters  war  veteran  comedy  \\\n",
       "0     0    0    0       0         0       0           0    0        0       0   \n",
       "1     0    0    0       0         0       0           0    0        0       0   \n",
       "2     0    0    0       0         0       0           0    0        0       0   \n",
       "3     0    0    0       0         0       0           0    0        0       0   \n",
       "4     0    0    0       0         0       0           0    0        0       0   \n",
       "\n",
       "   ...  tragic  hervé  leroux  léger  bandage  dress  30minute  100000  \\\n",
       "0  ...       0      0       0      0        0      0         0       0   \n",
       "1  ...       0      0       0      0        0      0         0       0   \n",
       "2  ...       0      0       0      0        0      0         0       0   \n",
       "3  ...       0      0       0      0        0      0         0       0   \n",
       "4  ...       0      0       0      0        0      0         0       0   \n",
       "\n",
       "   antiamerican  Label  \n",
       "0             0      1  \n",
       "1             0      1  \n",
       "2             0      1  \n",
       "3             0      1  \n",
       "4             0      1  \n",
       "\n",
       "[5 rows x 6635 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(data['Headline']) # tokenize and build vocabulary\n",
    "vector = vectorizer.transform(data['Headline']) # encode document\n",
    "print(\"Tamanho: \", vector.shape)\n",
    "head = [key for key in  vectorizer.vocabulary_]\n",
    "df = pd.DataFrame(vector.toarray(), columns = head)\n",
    "df['Label'] = data['Label']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Redução da Dimensionalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import svd \n",
    "\n",
    "def fisher_score(x, y):\n",
    "    mean = np.mean(x, axis=0)\n",
    "    classes = np.unique(y)\n",
    "    sS = 0\n",
    "    sD = 0\n",
    "    Nk = []\n",
    "    \n",
    "    meanD = []\n",
    "    varD = []\n",
    "    \n",
    "    for k in classes:\n",
    "        elements = []\n",
    "        s = 0\n",
    "        for i in range(0, len(y)):\n",
    "            if(y[i] == k):\n",
    "                s += 1\n",
    "                elements.append(x[i])\n",
    "        meanD.append(np.mean(elements, axis=0))\n",
    "        varD.append(np.var(elements, axis=0))\n",
    "        Nk.append(s)\n",
    "    \n",
    "    for k in range(0, len(classes)):\n",
    "        sS += (Nk[k] * ((meanD[k] - mean)**2))\n",
    "        sD += (Nk[k] * varD[k])\n",
    "    return sS/sD\n",
    "\n",
    "\n",
    "def compute(x):\n",
    "    mean = np.mean(x, axis = 0)\n",
    "    cov = np.cov(np.transpose(x))\n",
    "\n",
    "    U, S, V = svd(cov)\n",
    "    S = np.diag(S)\n",
    "    M = U @ S @ V\n",
    "    P = np.transpose(U)\n",
    "    return {'S': S, 'U': U, 'V': V, 'M': M, 'P': P}\n",
    "\n",
    "def transform(x, rateVariance):\n",
    "    pca_result = compute(x)\n",
    "    \n",
    "    S = pca_result['S']\n",
    "    P = pca_result['P']\n",
    "    \n",
    " #   autovalores = np.sort([S[i][i] for i in range(len(S))])[::-1]\n",
    "    #varianciaExplicadaTot = np.sum(np.array(autovalores))\n",
    "    #varianceExpl = rateVarience*varianciaExplicadaTot\n",
    "\n",
    "    #matrizTransform = []\n",
    "    \n",
    "    matrizTransform = []\n",
    "    varianciaExplicadaTot = 0\n",
    "    varianceExpl = 0\n",
    "    autoValores = []\n",
    "    for i in range(0, S.shape[0]):\n",
    "        varianciaExplicadaTot += S[i,i]\n",
    "        autoValores.append(S[i,i])\n",
    "    varianceExpl = rateVariance *varianciaExplicadaTot\n",
    "\n",
    "    autoValOrdenados = np.sort(autoValores)\n",
    "    autoValOrdenados = autoValOrdenados[::-1]\n",
    "    \n",
    "    varianceAtual = 0\n",
    "    i = 0\n",
    "    while(varianceAtual<=varianceExpl and i<autoValOrdenados.shape[0]):\n",
    "        for j in range (0, len(autoValores)):\n",
    "            if(autoValOrdenados[i]==autoValores[j]):\n",
    "                matrizTransform.append(P[j])\n",
    "        varianceAtual += autoValOrdenados[i]\n",
    "        i += 1\n",
    "    print(matrizTransform)\n",
    "    return {'P': matrizTransform, 'Z':np.transpose(np.dot(matrizTransform, np.transpose(x)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pemex</th>\n",
       "      <th>inspirational</th>\n",
       "      <th>come</th>\n",
       "      <th>fan</th>\n",
       "      <th>evacuation</th>\n",
       "      <th>af</th>\n",
       "      <th>role</th>\n",
       "      <th>karate</th>\n",
       "      <th>315</th>\n",
       "      <th>small</th>\n",
       "      <th>...</th>\n",
       "      <th>uae</th>\n",
       "      <th>valuable</th>\n",
       "      <th>suzi</th>\n",
       "      <th>fecal</th>\n",
       "      <th>poltergeist</th>\n",
       "      <th>2021</th>\n",
       "      <th>suspend</th>\n",
       "      <th>detention</th>\n",
       "      <th>relevant</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pemex  inspirational  come  fan  evacuation  af  role  karate  315  small  \\\n",
       "0      0              0     0    0           0   0     0       0    0      0   \n",
       "1      0              0     0    0           0   0     0       0    0      0   \n",
       "2      0              0     0    0           0   0     0       0    0      0   \n",
       "3      0              0     0    0           0   0     0       0    0      0   \n",
       "4      0              0     0    0           0   0     0       0    0      0   \n",
       "\n",
       "   ...  uae  valuable  suzi  fecal  poltergeist  2021  suspend  detention  \\\n",
       "0  ...    0         0     0      0            0     0        0          0   \n",
       "1  ...    0         0     0      0            0     0        0          0   \n",
       "2  ...    0         0     0      0            0     0        0          0   \n",
       "3  ...    0         0     0      0            0     0        0          0   \n",
       "4  ...    0         0     0      0            0     0        0          0   \n",
       "\n",
       "   relevant  Label  \n",
       "0         0      1  \n",
       "1         0      1  \n",
       "2         0      1  \n",
       "3         0      1  \n",
       "4         0      1  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fisher_scores = fisher_score(df.iloc[:,:-1].values, df['Label'].values)\n",
    "qnt = 6600 # quantas colunas quer dropar pelo fisher score\n",
    "indices = [] # armazena os indices dos menores valores do fisher_score -> que é o que vamos dropar\n",
    "for i in range(qnt): # faz o processamento de armazenar os indices dos menores valores\n",
    "    indice = np.argmin(fisher_scores)\n",
    "    fisher_scores[indice] = 10000\n",
    "    indices.append(indice)\n",
    "\n",
    "data_fisher = df.drop([df.columns[i] for i in indices] ,  axis='columns')\n",
    "data_fisher.head()\n",
    "#classes, ocorrencs = np.unique(indices, return_counts=True)\n",
    "#dict(zip(classes, ocorrencs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = transform(df, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pca = pd.DataFrame(teste['Z'])\n",
    "data_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Visualização e Análise dos dados\n",
    "\n",
    "- https://www.mien.in/2017/10/02/visual-text-analytics-with-python/\n",
    "- visualizações ta tudo aqui: https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a\n",
    "- o coisa la do kaggle https://www.kaggle.com/kumudchauhan/fake-news-analysis-and-classification#506957\n",
    "\n",
    "0. dataset.describe()\n",
    "0. dataset.corr()\n",
    "\n",
    "0. WordCloud\n",
    "\n",
    "1. Palavras mais frequentes por classe - sem stopwords - (grafico de barras)\n",
    "2. Palavras menos frequentes por classe - sem stopwords - (grafico de barras)\n",
    "\n",
    "3. Relação de stopwords em noticias reais/Fakes\n",
    "4. Relação da Escrita errada/correta em noticias reais/Fakes\n",
    "\n",
    "5. The distribution of top bigrams before removing stop words\n",
    "6. Analise de Sentimentos\n",
    "\n",
    "Which sources publish real news\n",
    "Which sources publish maximum fake news?\n",
    "3 Is there any common source which reports both real and fake news?\n",
    " Do sources include movies in the news to get more attention?\n",
    " Do sources include images in all the news as images provides visual cues?\n",
    " Most discriminatory words in the title of news\n",
    " Density distribuiton of title length for real and fake news\n",
    "\n",
    "\n",
    "Lexical dispersion plot\n",
    "Frequency distribution plot:\n",
    "Lexical diversity dispersion plot\n",
    "N-gram frequency distribution plot:\n",
    "Top 20 bigrams in review after removing stop words\n",
    "The distribution of Top trigrams before removing stop words\n",
    "The distribution of Top trigrams after removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pythondata.com/text-analytics-visualization/\n",
    "# https://doc.lagout.org/Others/Data%20Mining/Text%20Mining%20and%20Visualization_%20Case%20Studies%20using%20Open-Source%20Tools%20%5BHofmann%20%26%20Chisholm%202015-12-18%5D.pdf\n",
    "# https://itnext.io/basics-of-text-analysis-visualization-1978de48af47\n",
    "# https://kanoki.org/2019/03/17/text-data-visualization-in-python/\n",
    "# https://realpython.com/python-keras-text-classification/ (LER ISSO AQUI SEM DELONGAS)\n",
    "# https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/ (LER ISSO AQUI)\n",
    "# https://towardsdatascience.com/text-classification-in-python-dd95d264c802 (LER ISSO AQUI)\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#usar dataset pós-limpeza\n",
    "dataset = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titulosFake = \"\"\n",
    "titulosReal = \"\"\n",
    "for i in range(0, dataset.shape[0]):\n",
    "    if (dataset['Label'][i]==1):\n",
    "        titulosFake += dataset['Headline'][i]+\" \"\n",
    "    else:\n",
    "        titulosReal += dataset['Headline'][i]+\" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=100).generate(titulosFake)\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=100).generate(titulosReal)\n",
    "plt.pyplot.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.pyplot.axis(\"off\")\n",
    "plt.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsFake = titulosFake.split()\n",
    "wordsReal = titulosReal.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinctFake = []\n",
    "distinctReal = []\n",
    "for i in wordsFake:\n",
    "    if i not in distinctFake:\n",
    "        distinctFake.append(i)\n",
    "for i in wordsReal:\n",
    "    if i not in distinctReal:\n",
    "        distinctReal.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countFake = []\n",
    "countReal = []\n",
    "for i in distinctFake:\n",
    "    cont = 0\n",
    "    for j in wordsFake:\n",
    "        if (i==j):\n",
    "            cont += 1\n",
    "    countFake.append(cont)\n",
    "for i in distinctReal:\n",
    "    cont = 0\n",
    "    for j in wordsReal:\n",
    "        if (i==j):\n",
    "            cont += 1\n",
    "    countReal.append(cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.bar(distinctFake, countFake, color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.bar(distinctReal, countReal, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Avaliação de Métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x, y):\n",
    "    classes, ocorrencs = np.unique(y, return_counts=True) # pegando as classes e ocorrencias\n",
    "    numClasses = len(classes) # numero de classes\n",
    "    n = len(y) # numero de linhas do dataset\n",
    "    numFeatures = x.shape[1] # numero de colunas do dataset\n",
    "    \n",
    "    # Probabilidade das classes será proporcional a frequencia dessa classe no dataset\n",
    "    probabilidadeClasses = dict(zip(classes, ocorrencs))\n",
    "    for key in probabilidadeClasses:\n",
    "        probabilidadeClasses[key] = probabilidadeClasses[key] / n\n",
    "    \n",
    "    media = np.zeros((numFeatures, numClasses)) # criando a lista da media das features por classe\n",
    "    covar = np.zeros((numFeatures, numFeatures, numClasses)) # criando a lista de matrizes de correlação das features para cada classe\n",
    "\n",
    "    for classe in classes:\n",
    "        xk = x[y == classe] # pega as linhas em que classe é igual ao y -> que aí eu acesso essas linhas do x\n",
    "        classe = int(classe)\n",
    "        media[:, int(classe)] = np.mean(xk, axis=0)\n",
    "        xi_mean = xk - media[:, int(classe)]\n",
    "        covar[:, :, int(classe)] = (np.transpose(xi_mean) @ xi_mean)/len(xk)\n",
    "        covar[:, :, int(classe)] += np.eye(numFeatures) * np.mean(np.diag(covar[:,:,classe]))  * 10 ** -6 # ver se assim ta certo, pq o do cesar ele botou uns numeros que eu nao entendi\n",
    "    return {'media': media, 'covar': covar, 'classes': classes, 'numRows': n, 'numClasses': numClasses, 'numFeatures': numFeatures, 'probabilidadeClasses': probabilidadeClasses }\n",
    "        \n",
    "def predict(model, row):\n",
    "    probabilits = np.zeros(model['numClasses'])\n",
    "    for classe in model['classes']:\n",
    "        classe = int(classe)\n",
    "        fator1 = 1/(sqrt(np.linalg.det(model['covar'][:, :, classe])) * ((2*pi)**(model['numFeatures']/2)))\n",
    "        \n",
    "        inversa = np.linalg.inv(model['covar'][:, :, classe])\n",
    "        difXMedia = row - model['media'][:, classe]\n",
    "        z = (-0.5) * (np.transpose(difXMedia) @ inversa @ difXMedia) # valor que fica dentro do exp\n",
    "        probabilits[classe] = fator1 * exp(z)\n",
    "    return model['classes'][np.argmax(probabilits)]\n",
    "    \n",
    "    \n",
    "# Função para um vetor de testes e então chama o predict\n",
    "def getPredict(model, x_test):\n",
    "    predicted = np.array([predict(model, row) for row in x_test])\n",
    "    return predicted\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dos Dados\n",
    "# atribuindo X com as features e y com os valores de classe\n",
    "X = data_fisher.iloc[:,:-1].values\n",
    "Y = data_fisher['Label'].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-ff55319c7048>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetPredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-d46a78c2fce8>\u001b[0m in \u001b[0;36mgetPredict\u001b[1;34m(model, x_test)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m# Função para um vetor de testes e então chama o predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetPredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-d46a78c2fce8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m# Função para um vetor de testes e então chama o predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetPredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-d46a78c2fce8>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(model, row)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mclasse\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'classes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mclasse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mfator1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'covar'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'numFeatures'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0minversa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'covar'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "model = fit(x_train, y_train)\n",
    "results = getPredict(model, x_test)\n",
    "tot = [1 for x,z in zip(y_test,results) if x == z]\n",
    "(np.sum(np.array(tot))/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yPred(w, x):\n",
    "    yPred = 1/(1+np.exp(-x @ w))\n",
    "    return yPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regLogGD(x, y, n_epochs, alpha):\n",
    "    erroQM = []\n",
    "    wPrev = np.zeros(x.shape[1]+1)\n",
    "    aux = np.ones((x.shape[0], 1))\n",
    "    x = np.hstack((aux, x))\n",
    "    \n",
    "    for epochs in range (0,n_epochs):\n",
    "        suma = 0\n",
    "        sumErro = 0\n",
    "        for i in range(0, x.shape[0]):\n",
    "            sumErro = sumErro + (y[i]-yPred(wPrev,x[i]))**2\n",
    "            suma = suma +  (y[i]-yPred(wPrev,x[i]))*np.transpose(x[i])\n",
    "        w = np.transpose(wPrev) + alpha*(1/x.shape[0])*suma\n",
    "        erroEp = ((1/(2*x.shape[0]))*sumErro)\n",
    "        erroQM.append(erroEp)\n",
    "        wPrev = w\n",
    "    return wPrev, erroQM\n",
    "\n",
    "def predictLR(w, x):\n",
    "    yPredito = []\n",
    "    aux = np.ones((x.shape[0], 1))\n",
    "    x = np.hstack((aux, x))\n",
    "    for i in x:\n",
    "        if(yPred(w,i)>=0.5):\n",
    "            yPredito.append(1)\n",
    "        else:\n",
    "            yPredito.append(0)\n",
    "    return yPredito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5054835493519442"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, erro = regLogGD(x_train, y_train, 10, 0.001)\n",
    "x_train.shape\n",
    "results = predictLR(w, x_test)\n",
    "tot = [1 for x,z in zip(y_test,results) if x == z]\n",
    "(np.sum(np.array(tot))/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = linear_model.LogisticRegression(C = 5)\n",
    "logreg.fit(x_train, y_train)\n",
    "predictedRL = logreg.predict(x_test)\n",
    "score = accuracy_score(y_pred = predictedRL, y_true = y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = DistanceMetric.get_metric('euclidean')\n",
    "knn = KNeighborsClassifier(n_neighbors = 3, metric='euclidean')\n",
    "\n",
    "# treinando o modelo com os conjuntos de treino\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "# realizando a predicao\n",
    "predictedKNN = knn.predict(x_test)\n",
    "score = accuracy_score(y_pred = predictedKNN, y_true = y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Análise Discriminante Gaussiano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(x_train, y_train)\n",
    "predictedNB = clf.predict(x_test)\n",
    "score = accuracy_score(y_pred = predictedNB, y_true = y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. Árvore de Decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_classifier = DecisionTreeClassifier()\n",
    "tree_classifier.fit(x_train, y_train)\n",
    "predictedAD = tree_classifier.predict(x_test)\n",
    "score = accuracy_score(y_pred = predictedAD, y_true = y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5. MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params: \n",
    "    # hidden_layer_sizes \n",
    "    # activation : {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}\n",
    "    # alpha\n",
    "    # batch-size\n",
    "    # learning_rate\n",
    "    # momentum\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes = (300, 20))\n",
    "mlp.fit(x_train, y_train)\n",
    "\n",
    "# realizando a predicao\n",
    "predictedMLP = mlp.predict(x_test)\n",
    "score = accuracy_score(y_pred = predictedMLP, y_true = y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = svm.SVC(gamma='auto')\n",
    "#param_grid = [{'kernel': ['rbf'], 'C': 2 ** np.arange(-5.0, 16.0, 2),\n",
    "#                                  'gamma': 2 ** np.arange(-15.0, 4.0, 2)},\n",
    "#              {'kernel': ['poly'], 'C': 2 ** np.arange(-5.0, 16.0, 2),\n",
    "#                                   'degree': np.arange(2, 6)},\n",
    "#              {'kernel': ['linear'], 'C': 2 ** np.arange(-5.0, 16.0, 2)}]\n",
    "\n",
    "#model = GridSearchCV(model, param_grid, cv=3, n_jobs=-1)\n",
    "#model.fit(x_train, y_train) \n",
    "\n",
    "#support_vectors = model.best_estimator_.support_vectors_\n",
    "#support_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_arvores = 10\n",
    "max_columns = 3\n",
    "max_profundidade = 10\n",
    "clf = RandomForestClassifier(num_arvores,\"gini\",max_columns,max_profundidade)\n",
    "clf.fit(x_train, y_train)\n",
    "predictedRF = clf.predict(x_test)\n",
    "score = accuracy_score(y_pred = predictedRF, y_true = y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
